# Deployment for local development and for production

- [Configuration](#configuration)
   - [Environment variables](#environment-variables)
   - [Configuration file](#configuration-file)
- [Deploying locally for development](#deploying-locally-for-development)
   - [Introduction](#introduction)
   - [Instructions](#instructions)
- [Deploying a production instance](#deploying-a-production-instance)
   - [Overview](#overview)
   - [Prepare](#prepare)
   - [Start the container](#start-the-container)
   - [Stop the container](#stop-the-container)

(Table of contents automatically generated by
https://luciopaiva.com/markdown-toc/).

## Configuration

DVE is configured via two mechanisms:
- Environment variables for deployment parameters
- A configuration file (`config.yml`) for internal app configuration

### Environment variables

Environment variables configure major deployment parameters such as the
base path name and Gunicorn parameters.

`DASH_URL_BASE_PATHNAME`
- Base path for the app and all associated URLs including downloads

`LARGE_FILE_CACHE_SIZE`
- Number of large files (e.g., NetCDF data files) cached inside the app.

`SMALL_FILE_CACHE_SIZE`
- Number of small files (e.g., CSV files) cached inside the app.

`GUNICORN_<param>`
- GUNICORN configuration parameters. The Gunicorn configuration file
  (`docker/production/gunicorn.conf`) scrapes all environment variables
  of the form `GUNICORN_<param>` and sets value of configuration parameter
  `<param>` (converted to lowercase) to the value of the environment var.

### Configuration file

On startup, the app loads a configuration file (`config.yml`) containing
parameters used to direct the behaviour of the app. Configuration
parameters include:
- User interface defaults, settings, and labels (key `ui`)
- Filepaths to some data files (key `paths`)
- Filepaths and information about each design value (key `dvs`)
- Configuration of local preferences storage (key `local_config`)
- A miscellany of other values

Notes:

1. Filepaths are not absolute, and use `resource_pkgs.resource_filename`
set within `dve/` as a reference. This is not a desirable practice and
will 
[change](https://github.com/pacificclimate/dash-dv-explorer/issues/111) 
to absolute, direct filepaths at some point.

2. See the module docstring for module `dve/callbacks/local_preferences. py`
for details on how to configure local preferences. 

## Deploying locally for development

### Introduction

Using Docker for development is an expedient to avoid refactoring
the code to
[simplify its relationship to climpyrical](https://github.com/pacificclimate/dash-dv-explorer/issues/100) and
[improve the way data files are addressed](https://github.com/pacificclimate/dash-dv-explorer/issues/111). 
Avoiding the refactoring comes at some significant cost in Docker
complication, a cost which in the initial phases of this project was
worth the benefit, but which now outweighs it.

The `dve-dev-local` Docker image is the primary means for running the app while
developing and debugging. Using it has two advantages:
- It is very similar to the production environment.
- It reduces effort needed to install the supporting software.
- It maps a development configuration file onto `config.yml`.
  Typically, this configuration has a reduced dataset to speed startup, and
  uses local copies of large files ditto.

This image enables the developer to run the app in a Docker container, but
with "live" code updates visible inside the container. All infrastructure for
building and running this image is in `docker/dev-local`.

The image is normally built locally. (Because of this, there is no automated
build for the dev-local image as there is for the production image.) The image
installs the dependencies listed in `Pipfile` and `Pipfile.lock`, but does not 
install `dash-dev-explorer`. The image need only be rebuilt when project 
dependencies (`Pipfile`, `Pipfile.lock`) change.

After building, the image is run locally, and the
local codebase for `dash-dv-explorer` is mounted to it.
The container's first step (via the ENTRYPOINT) is to install that
local codebase.
With this arrangement, changes to the local codebase are available
directly inside the container. The container does not need to be restarted,
nor does the image need to be rebuilt in order to test code changes.

With the container running, the developer can run commands from inside it by
using `docker exec` commands. Most convenient is to use `docker exec` to run
an interactive `bash` shell in the container. From that bash shell all ordinary
commands can be run, including running tests and running the app.

### Instructions

1. **Advance prep**

   Do each of the following things *once per workstation*.

   1. Configure Docker user namespace mapping.

      1. Clone [`pdp-docker`](https://github.com/pacificclimate/pdp-docker).

      2. Follow the instructions in the `pdp-docker` documentation:
       [Setting up Docker namespace remapping (with recommended parameters)](https://github.com/pacificclimate/pdp-docker#setting-up-docker-namespace-remapping-with-recommended-parameters).

      1. Grant permissions on the downloads directory:

        ```
        setfacl -m "g:dockremap1000:rwx" docker/dev-local/downloads/ 
        ``` 

      4. Create and grant permissions on the DVE log file:

        ```
        touch dve_log.txt
        setfacl -m "g:dockremap1000:rw" dve_log.txt 
        ``` 
      
        Why is this necessary? This file is mounted to the `dve-dev-local` 
        Docker container. A file mount that does not exist is automatically 
        created by Docker, but it creates a directory, not a file. A file 
        must be 
        created in advance, and given suitable permissions if it is to be 
        written to.

   3. Update the development config (`docker/dev-local/config.yml`) as 
      needed.

   4. Copy any large datasets (e.g., reconstructions) to your local 
      codebase
     (typically under `local-data/`). This cuts app startup time from minutes
     to seconds. App startup is incurred every time you make a change to the
     codebase and want to see the results.

1. **Build the image**

   The image need only be (re)built when the project is first cloned and when
   `Pipenv` changes. To build the image:

    ```
    docker-compose -f docker/dev-local/docker-compose.yml build
    ```

   The image name is `pcic/dve-dev-local`.

1. **Start the container**

    ```
    docker-compose -f docker/dev-local/docker-compose.yml up -d
    ```

   The container name is `dve-dev-local`.

1. **Connect to a bash shell inside the container**

   When the container is running, you can connect to it and run a bash shell
   inside it with

    ```
    docker exec -it dve-dev-local bash
    ```

   You will see a prompt like

    ```
    dockremap@f4bcdc72b9f2:/codebase# 
    ```

   At this prompt you can enter bash commands, including the following:

1. **Start the app inside the container**

   From the container bash prompt:

    ```
    pipenv run python /codebase/dve.py --debug
    ```

   The `--debug` option does two things: Runs the server with `debug=True`, and
   defaults the logging level to `DEBUG`.

   Aside: Dash apps are based on Flask.
   Flask documentation
   [strongly recommends](https://flask.palletsprojects.com/en/1.1.x/server/#command-line)
   running apps for development using the Flask command line `flask run`.
   Unfortunately, that does not work for a Dash app, and we must run
   using a Python script as above.

   This enables the development environment, including the interactive debugger
   and reloader, and then starts the server on `http://localhost:5000/`.

   For more details, see the link above.

1. **Stop the container**

   When you have completed a cycle of development and testing, you may wish
   to stop the Docker container.

    ```
    docker-compose -f docker/dev-local/docker-compose.yml down
    ```

## Deploying a production instance

A production instance should be run in a production ready WSGI container
with proper process monitoring. We use [gunicorn](http://gunicorn.org/) as
the WSGI container.

### Overview

1. Creation of a deployment directory has been automated. The automation 
   creates a directory that:

   1. contains all the necessary artifacts for deployment (needing minimal 
      tweaking) using Docker Compose; and
   2. follows the standards for the structure of such directories that we 
      recently established for deployments.

   See below for details on how to use the automation.

2. A production Docker image, `pcic/design-value-explorer` is automatically 
   built on Dockerhub.
3. All production-related Docker infrastructure is in the repo under
   `docker/production`.
4. The following things are mounted to the Docker container and can be 
   updated locally as needed (requiring a restart of the container):
   1. Data files.
   2. App configuration files.
   3. App and Gunicorn logging configuration files.
5. The usual `docker-compose` commands can be used to start, stop, and restart
   the container.
6. Proxying:
  - Dash applications apparently know the domain they are proxied from.
    (Guessing this is via an HTTP header.)
  - They do not know the base URL path used by the proxy, so this must be
    specified using the environment variable `DASH_URL_BASE_PATHNAME` as
    noted below.
  - The proxy *must not* strip the base path from the URLs forwarded to the
    Dash app.

Details follow.

### Establish top-level deployment directory

IMPORTANT: You will not have to do this, because it is a do-once procedure that 
has already been done. But it is worth recording.

1. In a suitable location (at present `/storage/data/projects/comp_support/`)
   create a project deployment directory named `design-value-explorer/`.
2. In the project deployment directory, clone the project Git repository into 
   `repo`:
   ```
   cd design-value-explorer
   git clone https://github.com/pacificclimate/dash-dv-explorer.git repo
   ```

### Create a new deployment

Each time you wish to deploy a new release, make a new deployment (directory):

1. Determine what identifier tags the version you wish to deploy. 
   1. The identifier can be a tag (e.g., "2.3.0"), a branch name 
      (e.g., "i999-fix-all-bugs"), or even a commit SHA. 
   2. The identifier *must* be a valid ref in the Git repository.
   3. The identifier must refer to a commit coming after version (tag) 2.3.0.
      Otherwise, the deployment automation scripts will not be present.
   4. The identifier determines the name of the deployment directory and 
      what commit from the repo is used to obtain the deployment artifacts. 
2. From the project deployment root directory, run 
   `repo/deploy/make-deploy.sh prod <identifier>`.
   1. When it asks `Update deployment artifacts? [y,n]`, answer `y`.
   2. The script creates a deployment directory named `prod/<identifier>/`  
      and populates it with the necessary deployment artifacts 
      (configuration files, `docker-compose.yml`, etc.) copied from the 
      version of DVE identified by `<identifier>`. 
   3. These files are *copies* and can safely be modified and will not be 
      affected by other activities in the repo (e.g., subsequent deployments)
      . This directory is a permanent, unchanging record of the deployment.
3. Modify the deployment artifacts in the new directory 
   (`prod/<identifier>/`) as required. The project repo should generally be 
   up to date with typical or in-use configuration, but you _will_ need to do 
   the following:
   1. Verify and update if necessary the tag of the image for this 
      deployment. This should be the same as the `<identifier>` you selected 
      above, and some day I will think about automating this too.

   You _may_ need to do the following:

   1. Modify the logging configuration(s).
   2. Modify the data file mounts.
   3. Map a different port on the Docker container. 
   4. There are no secrets in the environment variables, but an 
      update would necessarily be required if secrets are 
      added in future.
4. Coordinate with IT as necessary for the new release.

### Stop the current services

1. From the date of this writing forward, there should be a similar deployment 
   directory for the current version. 
2. Find that version's identifier. Down the 
   old version using Docker Compose:
   ```
   cd prod/<previous version identifier>
   docker-compose -f docker/production/docker-compose.yml down
   ```
### Start the new services

1. Use Docker Compose to manage the new service:
   ```
   cd prod/<new version identifier>
   docker-compose -f docker/production/docker-compose.yml up -d
   docker-compose -f docker/production/docker-compose.yml down
   docker-compose -f docker/production/docker-compose.yml logs -f
   docker-compose -f docker/production/docker-compose.yml ps
   ```
   etc.

Note: The container name is `design-value-explorer`.

